{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4, Lab 1: Predicting Left-Handedness from Psychological Factors\n",
    "> Author: Matt Brems\n",
    "\n",
    "We can sketch out the data science process as follows:\n",
    "1. Define the problem.\n",
    "2. Obtain the data.\n",
    "3. Explore the data.\n",
    "4. Model the data.\n",
    "5. Evaluate the model.\n",
    "6. Answer the problem.\n",
    "\n",
    "We'll walk through a full data science problem in this lab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Define The Problem.\n",
    "\n",
    "You're currently a data scientist working at a university. A professor of psychology is attempting to study the relationship between personalities and left-handedness. They have tasked you with gathering evidence so that they may publish.\n",
    "\n",
    "Specifically, the professor says \"I need to prove that left-handedness is caused by some personality trait. Go find that personality trait and the data to back it up.\"\n",
    "\n",
    "As a data scientist, you know that any real data science problem must be **specific** and **conclusively answerable**. For example:\n",
    "- Bad data science problem: \"What is the link between obesity and blood pressure?\"\n",
    "    - This is vague and is not conclusively answerable. That is, two people might look at the conclusion and one may say \"Sure, the problem has been answered!\" and the other may say \"The problem has not yet been answered.\"\n",
    "- Good data science problem: \"Does an association exist between obesity and blood pressure?\"\n",
    "    - This is more specific and is conclusively answerable. The problem specifically is asking for a \"Yes\" or \"No\" answer. Based on that, two independent people should both be able to say either \"Yes, the problem has been answered\" or \"No, the problem has not yet been answered.\"\n",
    "- Excellent data science problem: \"As obesity increases, how does blood pressure change?\"\n",
    "    - This is very specific and is conclusively answerable. The problem specifically seeks to understand the effect of one variable on the other.\n",
    "\n",
    "### 1. In the context of the left-handedness and personality example, what are three specific and conclusively answerable problems that you could answer using data science? \n",
    "\n",
    "> You might find it helpful to check out the codebook in the repo for some inspiration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "1. As the degree to which indivudlas agree with the statement \"I would prefer a class in mathematics to a class in pottery\" increases, how does the liklihood of that individual being lef-handed change?\n",
    "2. As the degree to which indivudlas agree with the statement \"I have taken apart machines just to see how they work\" increases, how does the liklihood of that individual being lef-handed change?\n",
    "3. As the degree to which indivudlas agree with the statement \"I have studied how to win at gambling\" increases, how does the liklihood of that individual being lef-handed change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Obtain the data.\n",
    "\n",
    "### 2. Read in the file titled \"data.csv.\"\n",
    "> Hint: Despite being saved as a .csv file, you won't be able to simply `pd.read_csv()` this data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv\n",
    "\n",
    "df = pd.read_csv('./data.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Suppose that, instead of us giving you this data in a file, you were actually conducting a survey to gather this data yourself. From an ethics/privacy point of view, what are three things you might consider when attempting to gather this data?\n",
    "> When working with sensitive data like sexual orientation or gender identity, we need to consider how this data could be used if it fell into the wrong hands!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "1. All identifying information should be supressed. It should not be possible to use combinations of specific features to identify an individual.\n",
    "2. The purpose of the study and a clear explanation of how the data should be used should be clearly stated to all participants.\n",
    "3. If popssible, an Institutional Review Board (IRB) should be consulted to check for other sensitivities in the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Explore the data.\n",
    "\n",
    "### 4. Conduct exploratory data analysis on this dataset.\n",
    "> If you haven't already, be sure to check out the codebook in the repo, as that will help in your EDA process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5</th>\n",
       "      <th>Q6</th>\n",
       "      <th>Q7</th>\n",
       "      <th>Q8</th>\n",
       "      <th>Q9</th>\n",
       "      <th>Q10</th>\n",
       "      <th>...</th>\n",
       "      <th>country</th>\n",
       "      <th>fromgoogle</th>\n",
       "      <th>engnat</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>gender</th>\n",
       "      <th>orientation</th>\n",
       "      <th>race</th>\n",
       "      <th>religion</th>\n",
       "      <th>hand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>US</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>CA</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>NL</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>US</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>US</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Q1  Q2  Q3  Q4  Q5  Q6  Q7  Q8  Q9  Q10  ...  country  fromgoogle  engnat  \\\n",
       "0   4   1   5   1   5   1   5   1   4    1  ...       US           2       1   \n",
       "1   1   5   1   4   2   5   5   4   1    5  ...       CA           2       1   \n",
       "2   1   2   1   1   5   4   3   2   1    4  ...       NL           2       2   \n",
       "3   1   4   1   5   1   4   5   4   3    5  ...       US           2       1   \n",
       "4   5   1   5   1   5   1   5   1   3    1  ...       US           2       1   \n",
       "\n",
       "   age  education  gender  orientation  race  religion  hand  \n",
       "0   22          3       1            1     3         2     3  \n",
       "1   14          1       2            2     6         1     1  \n",
       "2   30          4       1            1     1         1     2  \n",
       "3   18          2       2            5     3         2     2  \n",
       "4   22          3       1            1     3         2     3  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4184 entries, 0 to 4183\n",
      "Data columns (total 56 columns):\n",
      "Q1             4184 non-null int64\n",
      "Q2             4184 non-null int64\n",
      "Q3             4184 non-null int64\n",
      "Q4             4184 non-null int64\n",
      "Q5             4184 non-null int64\n",
      "Q6             4184 non-null int64\n",
      "Q7             4184 non-null int64\n",
      "Q8             4184 non-null int64\n",
      "Q9             4184 non-null int64\n",
      "Q10            4184 non-null int64\n",
      "Q11            4184 non-null int64\n",
      "Q12            4184 non-null int64\n",
      "Q13            4184 non-null int64\n",
      "Q14            4184 non-null int64\n",
      "Q15            4184 non-null int64\n",
      "Q16            4184 non-null int64\n",
      "Q17            4184 non-null int64\n",
      "Q18            4184 non-null int64\n",
      "Q19            4184 non-null int64\n",
      "Q20            4184 non-null int64\n",
      "Q21            4184 non-null int64\n",
      "Q22            4184 non-null int64\n",
      "Q23            4184 non-null int64\n",
      "Q24            4184 non-null int64\n",
      "Q25            4184 non-null int64\n",
      "Q26            4184 non-null int64\n",
      "Q27            4184 non-null int64\n",
      "Q28            4184 non-null int64\n",
      "Q29            4184 non-null int64\n",
      "Q30            4184 non-null int64\n",
      "Q31            4184 non-null int64\n",
      "Q32            4184 non-null int64\n",
      "Q33            4184 non-null int64\n",
      "Q34            4184 non-null int64\n",
      "Q35            4184 non-null int64\n",
      "Q36            4184 non-null int64\n",
      "Q37            4184 non-null int64\n",
      "Q38            4184 non-null int64\n",
      "Q39            4184 non-null int64\n",
      "Q40            4184 non-null int64\n",
      "Q41            4184 non-null int64\n",
      "Q42            4184 non-null int64\n",
      "Q43            4184 non-null int64\n",
      "Q44            4184 non-null int64\n",
      "introelapse    4184 non-null int64\n",
      "testelapse     4184 non-null int64\n",
      "country        4184 non-null object\n",
      "fromgoogle     4184 non-null int64\n",
      "engnat         4184 non-null int64\n",
      "age            4184 non-null int64\n",
      "education      4184 non-null int64\n",
      "gender         4184 non-null int64\n",
      "orientation    4184 non-null int64\n",
      "race           4184 non-null int64\n",
      "religion       4184 non-null int64\n",
      "hand           4184 non-null int64\n",
      "dtypes: int64(55), object(1)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5</th>\n",
       "      <th>Q6</th>\n",
       "      <th>Q7</th>\n",
       "      <th>Q8</th>\n",
       "      <th>Q9</th>\n",
       "      <th>Q10</th>\n",
       "      <th>...</th>\n",
       "      <th>testelapse</th>\n",
       "      <th>fromgoogle</th>\n",
       "      <th>engnat</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>gender</th>\n",
       "      <th>orientation</th>\n",
       "      <th>race</th>\n",
       "      <th>religion</th>\n",
       "      <th>hand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.962715</td>\n",
       "      <td>3.829589</td>\n",
       "      <td>2.846558</td>\n",
       "      <td>3.186902</td>\n",
       "      <td>2.865440</td>\n",
       "      <td>3.672084</td>\n",
       "      <td>3.216539</td>\n",
       "      <td>3.184512</td>\n",
       "      <td>2.761233</td>\n",
       "      <td>3.522945</td>\n",
       "      <td>...</td>\n",
       "      <td>479.994503</td>\n",
       "      <td>1.576243</td>\n",
       "      <td>1.239962</td>\n",
       "      <td>30.370698</td>\n",
       "      <td>2.317878</td>\n",
       "      <td>1.654398</td>\n",
       "      <td>1.833413</td>\n",
       "      <td>5.013623</td>\n",
       "      <td>2.394359</td>\n",
       "      <td>1.190966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.360291</td>\n",
       "      <td>1.551683</td>\n",
       "      <td>1.664804</td>\n",
       "      <td>1.476879</td>\n",
       "      <td>1.545798</td>\n",
       "      <td>1.342238</td>\n",
       "      <td>1.490733</td>\n",
       "      <td>1.387382</td>\n",
       "      <td>1.511805</td>\n",
       "      <td>1.242890</td>\n",
       "      <td>...</td>\n",
       "      <td>3142.178542</td>\n",
       "      <td>0.494212</td>\n",
       "      <td>0.440882</td>\n",
       "      <td>367.201726</td>\n",
       "      <td>0.874264</td>\n",
       "      <td>0.640915</td>\n",
       "      <td>1.303454</td>\n",
       "      <td>1.970996</td>\n",
       "      <td>2.184164</td>\n",
       "      <td>0.495357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>186.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>242.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>324.250000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>119834.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>23763.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Q1           Q2           Q3           Q4           Q5  \\\n",
       "count  4184.000000  4184.000000  4184.000000  4184.000000  4184.000000   \n",
       "mean      1.962715     3.829589     2.846558     3.186902     2.865440   \n",
       "std       1.360291     1.551683     1.664804     1.476879     1.545798   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       1.000000     3.000000     1.000000     2.000000     1.000000   \n",
       "50%       1.000000     5.000000     3.000000     3.000000     3.000000   \n",
       "75%       3.000000     5.000000     5.000000     5.000000     4.000000   \n",
       "max       5.000000     5.000000     5.000000     5.000000     5.000000   \n",
       "\n",
       "                Q6           Q7           Q8           Q9          Q10  ...  \\\n",
       "count  4184.000000  4184.000000  4184.000000  4184.000000  4184.000000  ...   \n",
       "mean      3.672084     3.216539     3.184512     2.761233     3.522945  ...   \n",
       "std       1.342238     1.490733     1.387382     1.511805     1.242890  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       3.000000     2.000000     2.000000     1.000000     3.000000  ...   \n",
       "50%       4.000000     3.000000     3.000000     3.000000     4.000000  ...   \n",
       "75%       5.000000     5.000000     4.000000     4.000000     5.000000  ...   \n",
       "max       5.000000     5.000000     5.000000     5.000000     5.000000  ...   \n",
       "\n",
       "          testelapse   fromgoogle       engnat           age    education  \\\n",
       "count    4184.000000  4184.000000  4184.000000   4184.000000  4184.000000   \n",
       "mean      479.994503     1.576243     1.239962     30.370698     2.317878   \n",
       "std      3142.178542     0.494212     0.440882    367.201726     0.874264   \n",
       "min         7.000000     1.000000     0.000000     13.000000     0.000000   \n",
       "25%       186.000000     1.000000     1.000000     18.000000     2.000000   \n",
       "50%       242.000000     2.000000     1.000000     21.000000     2.000000   \n",
       "75%       324.250000     2.000000     1.000000     27.000000     3.000000   \n",
       "max    119834.000000     2.000000     2.000000  23763.000000     4.000000   \n",
       "\n",
       "            gender  orientation         race     religion         hand  \n",
       "count  4184.000000  4184.000000  4184.000000  4184.000000  4184.000000  \n",
       "mean      1.654398     1.833413     5.013623     2.394359     1.190966  \n",
       "std       0.640915     1.303454     1.970996     2.184164     0.495357  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       1.000000     1.000000     5.000000     1.000000     1.000000  \n",
       "50%       2.000000     1.000000     6.000000     2.000000     1.000000  \n",
       "75%       2.000000     2.000000     6.000000     2.000000     1.000000  \n",
       "max       3.000000     5.000000     7.000000     7.000000     3.000000  \n",
       "\n",
       "[8 rows x 55 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q1             0\n",
       "Q2             0\n",
       "Q3             0\n",
       "Q4             0\n",
       "Q5             0\n",
       "Q6             0\n",
       "Q7             0\n",
       "Q8             0\n",
       "Q9             0\n",
       "Q10            0\n",
       "Q11            0\n",
       "Q12            0\n",
       "Q13            0\n",
       "Q14            0\n",
       "Q15            0\n",
       "Q16            0\n",
       "Q17            0\n",
       "Q18            0\n",
       "Q19            0\n",
       "Q20            0\n",
       "Q21            0\n",
       "Q22            0\n",
       "Q23            0\n",
       "Q24            0\n",
       "Q25            0\n",
       "Q26            0\n",
       "Q27            0\n",
       "Q28            0\n",
       "Q29            0\n",
       "Q30            0\n",
       "Q31            0\n",
       "Q32            0\n",
       "Q33            0\n",
       "Q34            0\n",
       "Q35            0\n",
       "Q36            0\n",
       "Q37            0\n",
       "Q38            0\n",
       "Q39            0\n",
       "Q40            0\n",
       "Q41            0\n",
       "Q42            0\n",
       "Q43            0\n",
       "Q44            0\n",
       "introelapse    0\n",
       "testelapse     0\n",
       "country        0\n",
       "fromgoogle     0\n",
       "engnat         0\n",
       "age            0\n",
       "education      0\n",
       "gender         0\n",
       "orientation    0\n",
       "race           0\n",
       "religion       0\n",
       "hand           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Model the data.\n",
    "\n",
    "### 5. Suppose I wanted to use Q1 - Q44 to predict whether or not the person is left-handed. Would this be a classification or regression problem? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Classification. All of the questions are ordinal and discrete in nature and not continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. We want to use $k$-nearest neighbors to predict whether or not a person is left-handed based on their responses to Q1 - Q44. Before doing that, however, you remember that it is often a good idea to standardize your variables. In general, why would we standardize our variables? Give an example of when we would standardize our variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: We want to standardize our variables when they are on different scales. For example, comparing the number of rooms in a house to the number of square inches in the house might benefit from standardization, as their units of measurements are on very different scales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Give an example of when we might not standardize our variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Latitude and Longitude are already in the format we need for using them. That is, they are already standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Based on your answers to 6 and 7, do you think we should standardize our predictor variables in this case? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Probably not. All of the variables are already on an ordinal scale of 1-5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. We want to use $k$-nearest neighbors to predict whether or not a person is left-handed. What munging/cleaning do we need to do to our $y$ variable in order to explicitly answer this question? Do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: We should drop 'no-response' observations and dummify the variable for left-handedness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3542\n",
       "2     452\n",
       "3     179\n",
       "0      11\n",
       "Name: hand, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "df['hand'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4173 entries, 0 to 4183\n",
      "Data columns (total 56 columns):\n",
      "Q1             4173 non-null int64\n",
      "Q2             4173 non-null int64\n",
      "Q3             4173 non-null int64\n",
      "Q4             4173 non-null int64\n",
      "Q5             4173 non-null int64\n",
      "Q6             4173 non-null int64\n",
      "Q7             4173 non-null int64\n",
      "Q8             4173 non-null int64\n",
      "Q9             4173 non-null int64\n",
      "Q10            4173 non-null int64\n",
      "Q11            4173 non-null int64\n",
      "Q12            4173 non-null int64\n",
      "Q13            4173 non-null int64\n",
      "Q14            4173 non-null int64\n",
      "Q15            4173 non-null int64\n",
      "Q16            4173 non-null int64\n",
      "Q17            4173 non-null int64\n",
      "Q18            4173 non-null int64\n",
      "Q19            4173 non-null int64\n",
      "Q20            4173 non-null int64\n",
      "Q21            4173 non-null int64\n",
      "Q22            4173 non-null int64\n",
      "Q23            4173 non-null int64\n",
      "Q24            4173 non-null int64\n",
      "Q25            4173 non-null int64\n",
      "Q26            4173 non-null int64\n",
      "Q27            4173 non-null int64\n",
      "Q28            4173 non-null int64\n",
      "Q29            4173 non-null int64\n",
      "Q30            4173 non-null int64\n",
      "Q31            4173 non-null int64\n",
      "Q32            4173 non-null int64\n",
      "Q33            4173 non-null int64\n",
      "Q34            4173 non-null int64\n",
      "Q35            4173 non-null int64\n",
      "Q36            4173 non-null int64\n",
      "Q37            4173 non-null int64\n",
      "Q38            4173 non-null int64\n",
      "Q39            4173 non-null int64\n",
      "Q40            4173 non-null int64\n",
      "Q41            4173 non-null int64\n",
      "Q42            4173 non-null int64\n",
      "Q43            4173 non-null int64\n",
      "Q44            4173 non-null int64\n",
      "introelapse    4173 non-null int64\n",
      "testelapse     4173 non-null int64\n",
      "country        4173 non-null object\n",
      "fromgoogle     4173 non-null int64\n",
      "engnat         4173 non-null int64\n",
      "age            4173 non-null int64\n",
      "education      4173 non-null int64\n",
      "gender         4173 non-null int64\n",
      "orientation    4173 non-null int64\n",
      "race           4173 non-null int64\n",
      "religion       4173 non-null int64\n",
      "hand           4173 non-null int64\n",
      "dtypes: int64(55), object(1)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# drop no-response answers for left-handedness\n",
    "df = df.drop(df[(df['hand']==0)].index)\n",
    "# drop nulls if generated (this problem has presented itself a few times for me)\n",
    "df.dropna(inplace=True)\n",
    "# check for decrase of 11 in n\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      NaN\n",
       "1      NaN\n",
       "2      NaN\n",
       "3      NaN\n",
       "4      NaN\n",
       "        ..\n",
       "4179   NaN\n",
       "4180   NaN\n",
       "4181   NaN\n",
       "4182   NaN\n",
       "4183   NaN\n",
       "Name: hand, Length: 4173, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.hand.map({'Left': 1, 'Right': 0, 'Both': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3542\n",
       "2     452\n",
       "3     179\n",
       "Name: hand, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['hand'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. The professor for whom you work suggests that you set $k = 4$. In this specific case, why might this be a bad idea?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Picking an even number could result in a tie. Additionally, we probably want to choose our K value by running cross validation and determining which K value gives us the best score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Let's *(finally)* use $k$-nearest neighbors to predict whether or not a person is left-handed!\n",
    "\n",
    "> Be sure to create a train/test split with your data!\n",
    "\n",
    "> Create four separate models, one with $k = 3$, one with $k = 5$, one with $k = 15$, and one with $k = 25$.\n",
    "\n",
    "> Instantiate and fit your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['testelapse',     \n",
    "            'country',        \n",
    "            'fromgoogle',     \n",
    "            'engnat',         \n",
    "            'age',            \n",
    "            'education',      \n",
    "            'gender',         \n",
    "            'orientation',    \n",
    "            'race',           \n",
    "            'religion',       \n",
    "            'hand'],axis = 'columns')\n",
    "            \n",
    "y = df['hand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8063243450479233"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train_sc = ss.fit_transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)\n",
    "    \n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "\n",
    "cross_val_score(knn, X_train_sc, y_train, cv=5).mean()\n",
    "\n",
    "# Code adapted from GA DSI Lecture 4.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.8481943112815596\n",
      "test 0.8362068965517241\n"
     ]
    }
   ],
   "source": [
    "knn.fit(X_train_sc, y_train);\n",
    "print('train', knn.score(X_train_sc, y_train))\n",
    "print('test', knn.score(X_test_sc, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=3, 0.8063243450479233\n",
      "K=5, 0.8334921405750798\n",
      "K=15, 0.8485137380191693\n",
      "K=25, 0.8491532268370607\n"
     ]
    }
   ],
   "source": [
    "# for training data:\n",
    "\n",
    "for k in [3,5,15,25]:\n",
    "        model = KNeighborsClassifier(n_neighbors=k)\n",
    "        score = cross_val_score(model, X_train_sc, y_train, cv=5).mean()\n",
    "        print(f'K={k}, {score}')\n",
    "        \n",
    "# Code adapted from GA DSI Lecture 4.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=3, 0.8074806772175194\n",
      "K=5, 0.8371641516378359\n",
      "K=15, 0.8477042694147958\n",
      "K=25, 0.8477042694147958\n"
     ]
    }
   ],
   "source": [
    "# for test data:\n",
    "\n",
    "for k in [3,5,15,25]:\n",
    "        model = KNeighborsClassifier(n_neighbors=k)\n",
    "        score = cross_val_score(model, X_test_sc, y_test, cv=5).mean()\n",
    "        print(f'K={k}, {score}')\n",
    "        \n",
    "# Code adapted from GA DSI Lecture 4.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being good data scientists, we know that we might not run just one type of model. We might run many different models and see which is best.\n",
    "\n",
    "### 12. We want to use logistic regression to predict whether or not a person is left-handed. Before we do that, let's check the [documentation for logistic regression in sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Is there default regularization? If so, what is it? If not, how do you know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Yep! From the documentation: \"Note that regularization is applied by default.\" The documentation describing parameters states \"penalty{‘l1’, ‘l2’, ‘elasticnet’, ‘none’}, default=’l2.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. We want to use logistic regression to predict whether or not a person is left-handed. Before we do that, should we standardize our features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Logistic regression has built in standardization in the solver argument "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Let's use logistic regression to predict whether or not the person is left-handed.\n",
    "\n",
    "\n",
    "> Be sure to use the same train/test split with your data as with your $k$-NN model above!\n",
    "\n",
    "> Create four separate models, one with LASSO and $\\alpha = 1$, one with LASSO and $\\alpha = 10$, one with Ridge and $\\alpha = 1$, and one with Ridge and $\\alpha = 10$. *(Hint: Be careful with how you specify $\\alpha$ in your model!)*\n",
    "\n",
    "> Instantiate and fit your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import Lasso, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummify\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "X_train_dummified = ohe.fit_transform(X_train, y_train)\n",
    "X_test_dummified = ohe.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Standard Scaler to dummified X\n",
    "sc = StandardScaler()\n",
    "X_train_sc = sc.fit_transform(X_train_dummified)\n",
    "X_test_sc = sc.transform(X_test_dummified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate: # default alpha is 1\n",
    "ridge = Ridge()        \n",
    "lasso = Lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0012918611458810681, -0.0012918611458810681, -0.0012918611458810681, -0.0012918611458810681, -0.0012918611458810681]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, -0.0012918611458810681), (10, -0.0012918611458810681)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make Alpha List:\n",
    "alpha_list = [1, 10]\n",
    "# Instantiate Mean Scores:\n",
    "mean_score_list = []\n",
    "# Function:\n",
    "for a_value in a_list:\n",
    "    lasso = Lasso(alpha=a_value)\n",
    "    #ridge = Ridge(alpha=a_value)\n",
    "    mean_score = cross_val_score(lasso, X_train_sc, y_train).mean()\n",
    "    #mean_score = cross_val_score(ridge, X_train_sc, y_train).mean()\n",
    "    mean_score_list.append(mean_score)\n",
    "\n",
    "print(mean_score_list)\n",
    "list(zip(alpha_list, mean_score_list))\n",
    "\n",
    "# Code adapted from GA DSI Local Lesson on GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00012331508175300598"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso.fit(X_train_sc, y_train)\n",
    "lasso.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Well, these seem awfully wrong to me :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Evaluate the model(s).\n",
    "\n",
    "### 15. Before calculating any score on your data, take a step back. Think about your $X$ variable and your $Y$ variable. Do you think your $X$ variables will do a good job of predicting your $Y$ variable? Why or why not? What impact do you think this will have on your scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "Probably not! At least, not in the case of the logistic regressions. In retreospet, dummifying each ordinal value may not have been as prudent as creating simple binary encodings for each question rather than each ordinal answer to each question. This apprach will likely result in poor logistic regression results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Using accuracy as your metric, evaluate all eight of your models on both the training and testing sets. Put your scores below. (If you want to be fancy and generate a table in Markdown, there's a [Markdown table generator site linked here](https://www.tablesgenerator.com/markdown_tables#).)\n",
    "- Note: Your answers here might look a little weird. You didn't do anything wrong; that's to be expected!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: They sure do look weird! Need to return to this and figure out what's going on. I thought with LASSO and Ridge, we need to one-hot-encode and scale when given so many ordinal values? Could use some help.  \n",
    "\n",
    "|  Model   |     score     | k/alpha\n",
    "|----------|:-------------:|------:|\n",
    "|   KNN    |       0.80748 |   3   |\n",
    "|   KNN    |       0.83716 |   5   |\n",
    "|   KNN    |       0.84770 |  15   |\n",
    "|   KNN    |       0.84770 |  25   |\n",
    "|  LASSO   |       0.00129 |   1   |\n",
    "|  LASSO   |       0.00129 |  10   |\n",
    "|  RIDGE   |       0.00452 |   1   |\n",
    "|  RIDGE   |       0.00451 |  10   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. In which of your $k$-NN models is there evidence of overfitting? How do you know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Though none of these models display drastic differences between the train and test scores, the KNN model with a k of 25 shows the greatest discrepency between train and test scores, suggesting overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. Broadly speaking, how does the value of $k$ in $k$-NN affect the bias-variance tradeoff? (i.e. As $k$ increases, how are bias and variance affected?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: As K increases, we generally expect bias (training error) to increase and variance (test error) to decrease.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. If you have a $k$-NN model that has evidence of overfitting, what are three things you might try to do to combat overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "1. Attempt a different distance metric (Euclidean or Manhattan)\n",
    "2. Hypertune K: attempt to optimize K by looking for the value of K that provides the best cross-val score.\n",
    "3. Regularize our model by adding a penalty term to our loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. In which of your logistic regression models is there evidence of overfitting? How do you know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Need to come back to this questions. One might have expected overfitting to manifest in more significant vairation between the training and test results, but given the model scores, it might be worth reassessing the aproach to one-hot encoding and standard scaler before assessing overfit. Further investigation is required. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21. Broadly speaking, how does the value of $C$ in logistic regression affect the bias-variance tradeoff? (i.e. As $C$ increases, how are bias and variance affected?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "Assuming that by C we're talking about Alpha (I don't recall that nuance being covered in class?)\n",
    "The Alpha hyperparameter controls the size of the coefficient and  amount of regularization in a model. As Alpha increases, we would expect bias to increase, though this might not effect a models' accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22. For your logistic regression models, play around with the regularization hyperparameter, $C$. As you vary $C$, what happens to the fit and coefficients in the model? What do you think this means in the context of this specific problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Need to return to this, given the curious Ridge and LASSO model results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23. If you have a logistic regression model that has evidence of overfitting, what are three things you might try to do to combat overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "1. Attempt using L1 and L2 models (Ridge and LASSO) to better acount for sparse or multicolinear data.\n",
    "2. Hypertune Alpha: attempt to optimize alpha by looking for the value that provides the best cross-val score.\n",
    "3. Regularize our model by adding a penalty term to our loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Answer the problem.\n",
    "\n",
    "### 24. Suppose you want to understand which psychological features are most important in determining left-handedness. Would you rather use $k$-NN or logistic regression? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Probably a KNN, based on the model results. Since KNN assign a class (left-handedness) to unclassified_point using \"votes\" from k_nearest_points, it seems an reasonable tool for this data science questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25. Select your logistic regression model that utilized LASSO regularization with $\\alpha = 1$. Interpret the coefficient for `Q1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -0., -0.,  0.,  0.,  0., -0.,  0., -0., -0.,  0., -0.,  0.,\n",
       "       -0., -0., -0., -0.,  0.,  0., -0., -0.,  0.,  0.,  0.,  0., -0.,\n",
       "        0.,  0., -0.,  0.,  0.,  0.,  0., -0., -0., -0., -0., -0., -0.,\n",
       "        0., -0.,  0.,  0.,  0.,  0.,  0., -0., -0.,  0., -0., -0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0., -0.,  0., -0., -0., -0.,  0., -0., -0.,\n",
       "        0.,  0.,  0., -0., -0.,  0., -0.,  0., -0., -0., -0.,  0.,  0.,\n",
       "        0.,  0., -0.,  0.,  0., -0.,  0., -0.,  0.,  0., -0.,  0.,  0.,\n",
       "       -0., -0.,  0.,  0., -0.,  0., -0.,  0., -0.,  0.,  0.,  0.,  0.,\n",
       "       -0.,  0.,  0., -0.,  0., -0., -0., -0., -0.,  0.,  0.,  0.,  0.,\n",
       "       -0., -0., -0.,  0.,  0., -0.,  0., -0., -0.,  0.,  0., -0.,  0.,\n",
       "       -0., -0., -0., -0., -0.,  0.,  0., -0.,  0.,  0., -0.,  0., -0.,\n",
       "       -0., -0., -0., -0.,  0.,  0.,  0.,  0., -0.,  0., -0., -0.,  0.,\n",
       "        0., -0., -0.,  0.,  0.,  0.,  0.,  0., -0., -0.,  0., -0.,  0.,\n",
       "       -0., -0.,  0., -0.,  0., -0.,  0.,  0.,  0., -0., -0.,  0.,  0.,\n",
       "       -0., -0., -0.,  0.,  0.,  0., -0., -0.,  0., -0.,  0., -0.,  0.,\n",
       "       -0.,  0.,  0.,  0.,  0., -0., -0., -0.,  0., -0., -0.,  0., -0.,\n",
       "        0.,  0.,  0.,  0.,  0., -0., -0.,  0.,  0., -0.,  0., -0.,  0.,\n",
       "        0.,  0.,  0., -0., -0., -0.,  0.,  0., -0., -0.,  0., -0.,  0.,\n",
       "        0.,  0.,  0., -0.,  0., -0.,  0.,  0., -0.,  0., -0.,  0.,  0.,\n",
       "       -0., -0., -0., -0.,  0.,  0., -0., -0.,  0.,  0.,  0.,  0., -0.,\n",
       "       -0., -0., -0.,  0., -0.,  0.,  0.,  0.,  0., -0., -0.,  0.,  0.,\n",
       "        0.,  0., -0.,  0.,  0.,  0.,  0., -0., -0., -0., -0., -0., -0.,\n",
       "        0., -0.,  0.,  0., -0., -0.,  0.,  0., -0., -0.,  0.,  0., -0.,\n",
       "        0., -0.,  0., -0.,  0., -0., -0., -0., -0., -0., -0., -0.,  0.,\n",
       "       -0., -0., -0.,  0., -0., -0., -0.,  0.,  0., -0.,  0., -0., -0.,\n",
       "       -0.,  0., -0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0.,  0.,\n",
       "       -0., -0., -0., -0., -0.,  0.,  0., -0., -0.,  0., -0., -0., -0.,\n",
       "        0., -0., -0., -0., -0., -0.,  0., -0.,  0., -0., -0.,  0., -0.,\n",
       "       -0.,  0., -0., -0., -0., -0., -0., -0.,  0.,  0., -0., -0., -0.,\n",
       "       -0., -0., -0.,  0., -0., -0., -0.,  0., -0., -0., -0.,  0.,  0.,\n",
       "       -0., -0., -0.,  0.,  0., -0., -0., -0.,  0., -0., -0.,  0., -0.,\n",
       "       -0., -0., -0., -0.,  0., -0., -0., -0., -0., -0., -0., -0.,  0.,\n",
       "       -0.,  0., -0., -0., -0.,  0., -0., -0., -0., -0., -0., -0.,  0.,\n",
       "       -0.,  0.,  0., -0., -0., -0., -0., -0.,  0.,  0., -0., -0., -0.,\n",
       "       -0., -0.,  0., -0., -0.,  0., -0., -0.,  0.,  0., -0., -0., -0.,\n",
       "        0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,\n",
       "       -0., -0., -0., -0., -0., -0.,  0., -0., -0., -0., -0., -0., -0.,\n",
       "       -0., -0.,  0., -0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "       -0., -0., -0.,  0., -0., -0.,  0., -0., -0., -0., -0., -0., -0.,\n",
       "       -0., -0.,  0., -0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "       -0., -0., -0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0., -0.,\n",
       "       -0., -0., -0., -0.,  0., -0.,  0., -0., -0., -0., -0., -0.,  0.,\n",
       "       -0., -0.,  0., -0., -0., -0.,  0., -0., -0., -0., -0., -0., -0.,\n",
       "       -0., -0.,  0.,  0., -0.,  0., -0.,  0.,  0., -0., -0., -0., -0.,\n",
       "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "       -0., -0., -0., -0., -0., -0.,  0., -0., -0., -0., -0.,  0., -0.,\n",
       "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "       -0., -0., -0.])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso.coef_\n",
    "\n",
    "#feature_cols = X_train_sc.columns\n",
    "\n",
    "#df_lr = pd.DataFrame([lasso.coef_], columns=feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 26. If you have to select one model overall to be your *best* model, which model would you select? Why?\n",
    "- Usually in the \"real world,\" you'll fit many types of models but ultimately need to pick only one! (For example, a client may not understand what it means to have multiple models, or if you're using an algorithm to make a decision, it's probably pretty challenging to use two or more algorithms simultaneously.) It's not always an easy choice, but you'll have to make it soon enough. Pick a model and defend why you picked this model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 27. Circle back to the three specific and conclusively answerable questions you came up with in Q1. Answer one of these for the professor based on the model you selected!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KNeighborsClassifier' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-150-f2cbc09e1acf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'KNeighborsClassifier' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS:\n",
    "Looking for more to do? Probably not - you're busy! But if you want to, consider exploring the following. (They could make for a blog post!)\n",
    "- Create a visual plot comparing training and test metrics for various values of $k$ and various regularization schemes in logistic regression.\n",
    "- Rather than just evaluating models based on accuracy, consider using sensitivity, specificity, etc.\n",
    "- In the context of predicting left-handedness, why are unbalanced classes concerning? If you were to re-do this process given those concerns, what changes might you make?\n",
    "- Fit and evaluate a generalized linear model other than logistic regression (e.g. Poisson regression).\n",
    "- Suppose this data were in a `SQL` database named `data` and a table named `inventory`. What `SQL` query would return the count of people who were right-handed, left-handed, both, or missing with their class labels of 1, 2, 3, and 0, respectively? (You can assume you've already logged into the database.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
