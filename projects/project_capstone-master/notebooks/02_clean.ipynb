{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook cleans the collected and merged ACS data, preparing it for the EDA process. Noteable outliers regarding data consistency are Census Tracts including the National Mall, Georgetown University, and two DC aggregate observations included in the ACS data-download. Given a population of 60, the National Mall Census Tract and DC aggregate observations are dropped. The Georgetown University Census Tract is kept, but the missing values are converted to zeros. This will not present an issiue with clustering, and given its status as the only Census Tract in DC that is specific to a university and not surrounding residential areas, converting missing values to zeros will not interfere in the modeling process. I.e., the models will still capture the Census Tract as an outlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data, set index to 'geo_id':\n",
    "\n",
    "df = pd.read_csv('../data/outputs/01_merged.csv', index_col=False)\n",
    "df = df.set_index('geo_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Unneeded Observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop observations aggregating DC data (ACS includes aggregate DC data in download): \n",
    "# Aggregated DC observations by geo_id index = ['0400000US11', '1600000US1150000']\n",
    "# Census Tracts to drop -- DC Mall/Whitehouse = ['1400000US11001006202']\n",
    "\n",
    "df = df.drop(index = ['0400000US11', '1600000US1150000', '1400000US11001006202'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Duplicate Name Columns from Merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['name_y', 'name_x.1', 'name_y.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 178 entries, 1400000US11001000100 to 1400000US11001011100\n",
      "Data columns (total 26 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   name_x                    178 non-null    object \n",
      " 1   total_pop                 178 non-null    int64  \n",
      " 2   pct_male                  178 non-null    float64\n",
      " 3   median_age                178 non-null    object \n",
      " 4   pct_hisp_latino           178 non-null    float64\n",
      " 5   pce_white                 178 non-null    float64\n",
      " 6   pct_black                 178 non-null    float64\n",
      " 7   pct_american_ind          178 non-null    float64\n",
      " 8   pct_asian                 178 non-null    float64\n",
      " 9   pct_hawaiian_pacisldr     178 non-null    float64\n",
      " 10  pct_other_race            178 non-null    float64\n",
      " 11  pct_unemployed            178 non-null    float64\n",
      " 12  avg_wrk_commute_mins      178 non-null    object \n",
      " 13  median_hsld_income        178 non-null    object \n",
      " 14  per_cap_income            178 non-null    object \n",
      " 15  total_households          178 non-null    int64  \n",
      " 16  avg_household_size        178 non-null    object \n",
      " 17  avg_family_size           178 non-null    object \n",
      " 18  pop_in_households         178 non-null    int64  \n",
      " 19  pct_bach_degree           178 non-null    float64\n",
      " 20  total_housing_units       178 non-null    int64  \n",
      " 21  occupied_housing_units    178 non-null    int64  \n",
      " 22  total_units_in_structure  178 non-null    int64  \n",
      " 23  units_built_after_2014    178 non-null    int64  \n",
      " 24  owner_occupied_units      178 non-null    int64  \n",
      " 25  renter_occupied_units     178 non-null    int64  \n",
      "dtypes: float64(10), int64(9), object(7)\n",
      "memory usage: 37.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Non-Numeric Characters from Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove + character from 'median_hsld_income' column:\n",
    "\n",
    "df['median_hsld_income'] = df['median_hsld_income'].str.rstrip('+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Georgetown Unioversity Census Tract missing data with '0.' For clustering purposes, this will suffice. \n",
    "\n",
    "df = df.replace('-', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove commas from ACS 'median_hsld_income' column:\n",
    "\n",
    "df['median_hsld_income'] = df['median_hsld_income'].replace(',','', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Columns to Numeric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to numeric:\n",
    "\n",
    "df[['median_age', \n",
    "    'median_hsld_income', \n",
    "    'avg_wrk_commute_mins',\n",
    "    'per_cap_income',\n",
    "    'avg_household_size',\n",
    "    'avg_family_size']] = df[['median_age', \n",
    "                              'median_hsld_income', \n",
    "                              'avg_wrk_commute_mins',\n",
    "                              'per_cap_income',\n",
    "                              'avg_household_size',\n",
    "                              'avg_family_size']].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 178 entries, 1400000US11001000100 to 1400000US11001011100\n",
      "Data columns (total 26 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   name_x                    178 non-null    object \n",
      " 1   total_pop                 178 non-null    int64  \n",
      " 2   pct_male                  178 non-null    float64\n",
      " 3   median_age                178 non-null    float64\n",
      " 4   pct_hisp_latino           178 non-null    float64\n",
      " 5   pce_white                 178 non-null    float64\n",
      " 6   pct_black                 178 non-null    float64\n",
      " 7   pct_american_ind          178 non-null    float64\n",
      " 8   pct_asian                 178 non-null    float64\n",
      " 9   pct_hawaiian_pacisldr     178 non-null    float64\n",
      " 10  pct_other_race            178 non-null    float64\n",
      " 11  pct_unemployed            178 non-null    float64\n",
      " 12  avg_wrk_commute_mins      178 non-null    float64\n",
      " 13  median_hsld_income        178 non-null    int64  \n",
      " 14  per_cap_income            178 non-null    int64  \n",
      " 15  total_households          178 non-null    int64  \n",
      " 16  avg_household_size        178 non-null    float64\n",
      " 17  avg_family_size           178 non-null    float64\n",
      " 18  pop_in_households         178 non-null    int64  \n",
      " 19  pct_bach_degree           178 non-null    float64\n",
      " 20  total_housing_units       178 non-null    int64  \n",
      " 21  occupied_housing_units    178 non-null    int64  \n",
      " 22  total_units_in_structure  178 non-null    int64  \n",
      " 23  units_built_after_2014    178 non-null    int64  \n",
      " 24  owner_occupied_units      178 non-null    int64  \n",
      " 25  renter_occupied_units     178 non-null    int64  \n",
      "dtypes: float64(14), int64(11), object(1)\n",
      "memory usage: 37.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Ensure fields are numeric, and 178 entries, no null values, and drop any existing duplicates:\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Clean Data for EDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export without index:\n",
    "\n",
    "df.to_csv('../data/outputs/02_merged_clean.csv', index='geo_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA conducted in notebook '03_eda.ipynb'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
