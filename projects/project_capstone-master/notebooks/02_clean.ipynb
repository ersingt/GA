{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook cleans the collected and merged ACS data, preparing it for the EDA process. Noteable outliers regarding data consistency are Census Tracts including the National Mall, Georgetown University, and two DC aggregate observations included in the ACS data-download. Given a population of 60, the National Mall Census Tract and DC aggregate observations are dropped. The Georgetown University Census Tract is kept, but the missing values are converted to zeros. This will not present an issiue with clustering, and given its status as the only Census Tract in DC that is specific to a university and not surrounding residential areas, converting missing values to zeros will not interfere in the modeling process. I.e., the models will still capture the Census Tract as an outlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data, set index to 'geo_id':\n",
    "\n",
    "df = pd.read_csv('../data/outputs/01_merged.csv', index_col=False)\n",
    "df = df.set_index('geo_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Unneeded Observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop observations aggregating DC data (ACS includes aggregate DC data in download): \n",
    "# Aggregated DC observations by geo_id index = ['0400000US11', '1600000US1150000']\n",
    "# Census Tracts to drop -- DC Mall/Whitehouse, Georgetown University = ['1400000US11001006202', 1400000US11001000201]\n",
    "\n",
    "df = df.drop(index = ['0400000US11', '1600000US1150000', '1400000US11001006202', '1400000US11001000201'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Duplicate Name Columns from Merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['name_y', 'name_x.1', 'name_y.1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Non-Numeric Characters from Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove + character from 'median_hsld_income' and 'median_rent' columns:\n",
    "\n",
    "df['median_hsld_income'] = df['median_hsld_income'].str.rstrip('+')\n",
    "df['median_rent'] = df['median_rent'].str.rstrip('+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Census Tract missing data with '0.' For clustering purposes, this will suffice. \n",
    "\n",
    "df = df.replace('-', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove commas from ACS 'median_hsld_income' and 'median_rent' columns:\n",
    "\n",
    "df['median_hsld_income'] = df['median_hsld_income'].replace(',','', regex=True)\n",
    "df['median_rent'] = df['median_rent'].replace(',','', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Columns to Numeric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to numeric:\n",
    "\n",
    "df[['median_age', \n",
    "    'median_hsld_income', \n",
    "    'avg_wrk_commute_mins',\n",
    "    'per_cap_income',\n",
    "    'avg_household_size',\n",
    "    'avg_family_size',\n",
    "    'median_rent',\n",
    "    'median_home_value']] = df[['median_age', \n",
    "                              'median_hsld_income', \n",
    "                              'avg_wrk_commute_mins',\n",
    "                              'per_cap_income',\n",
    "                              'avg_household_size',\n",
    "                              'avg_family_size',\n",
    "                              'median_rent',\n",
    "                              'median_home_value']].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 177 entries, 1400000US11001000100 to 1400000US11001011100\n",
      "Data columns (total 28 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   name_x                    177 non-null    object \n",
      " 1   total_pop                 177 non-null    int64  \n",
      " 2   pct_male                  177 non-null    float64\n",
      " 3   median_age                177 non-null    float64\n",
      " 4   pct_hisp_latino           177 non-null    float64\n",
      " 5   pct_white                 177 non-null    float64\n",
      " 6   pct_black                 177 non-null    float64\n",
      " 7   pct_american_ind          177 non-null    float64\n",
      " 8   pct_asian                 177 non-null    float64\n",
      " 9   pct_hawaiian_pacisldr     177 non-null    float64\n",
      " 10  pct_other_race            177 non-null    float64\n",
      " 11  pct_unemployed            177 non-null    float64\n",
      " 12  avg_wrk_commute_mins      177 non-null    float64\n",
      " 13  median_hsld_income        177 non-null    int64  \n",
      " 14  per_cap_income            177 non-null    int64  \n",
      " 15  total_households          177 non-null    int64  \n",
      " 16  avg_household_size        177 non-null    float64\n",
      " 17  avg_family_size           177 non-null    float64\n",
      " 18  pop_in_households         177 non-null    int64  \n",
      " 19  pct_bach_degree           177 non-null    float64\n",
      " 20  total_housing_units       177 non-null    int64  \n",
      " 21  occupied_housing_units    177 non-null    int64  \n",
      " 22  total_units_in_structure  177 non-null    int64  \n",
      " 23  units_built_after_2014    177 non-null    int64  \n",
      " 24  owner_occupied_units      177 non-null    int64  \n",
      " 25  renter_occupied_units     177 non-null    int64  \n",
      " 26  median_rent               177 non-null    int64  \n",
      " 27  median_home_value         177 non-null    int64  \n",
      "dtypes: float64(14), int64(13), object(1)\n",
      "memory usage: 40.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Ensure fields are numeric, and 178 entries, no null values, and drop any existing duplicates:\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Clean Data for EDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export without index:\n",
    "\n",
    "df.to_csv('../data/outputs/02_merged_clean.csv', index='geo_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA conducted in notebook '03_eda.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
